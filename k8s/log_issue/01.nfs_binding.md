Name:             mysql-statefulset-0
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker1/192.168.56.22
Start Time:       Sat, 05 Apr 2025 08:59:51 +0000
Labels:           app=mysql
                  apps.kubernetes.io/pod-index=0
                  controller-revision-hash=mysql-statefulset-66d6f94f74
                  env=production
                  name=mysql-pod
                  statefulset.kubernetes.io/pod-name=mysql-statefulset-0
                  tier=database
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    StatefulSet/mysql-statefulset
Init Containers:
  init-mysql:
    Container ID:  
    Image:         mysql:5.7
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
      -c
      set -ex
      [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1
      ordinal=${BASH_REMATCH[1]}
      echo [mysqld] > /mnt/conf.d/server-id.cnf
      echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
      if [[ $ordinal -eq 0 ]]; then
        cp /mnt/mysql-configmap/master.cnf /mnt/conf.d/
      else
        cp /mnt/mysql-configmap/slave.cnf /mnt/conf.d/
      fi
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /mnt/conf.d from mysql-conf (rw)
      /mnt/mysql-configmap from mysql-configmap (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m45lh (ro)
  clone-mysql:
    Container ID:  
    Image:         gcr.io/google-samples/xtrabackup:1.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
      -c
      set -ex
      [[ -d /var/lib/mysql/mysql ]] && exit 0
      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
      ordinal=${BASH_REMATCH[1]}
      [[ $ordinal -eq 0 ]] && exit 0
      ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
      xtrabackup --prepare --target-dir=/var/lib/mysql
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/mysql/conf.d from mysql-conf (rw)
      /var/lib/mysql from mysql-data (rw,path="mysql")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m45lh (ro)
Containers:
  mysql:
    Container ID:   
    Image:          mysql:5.7
    Image ID:       
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Readiness:      exec [mysql -h 127.0.0.1 -e SELECT 1] delay=30s timeout=1s period=2s #success=1 #failure=3
    Startup:        exec [mysqladmin ping] delay=10s timeout=1s period=2s #success=1 #failure=3
    Environment:
      MYSQL_USER:                  <set to the key 'mysql-user' of config map 'mysql-configmap'>                  Optional: false
      MYSQL_DATABASE:              <set to the key 'mysql-database' of config map 'mysql-configmap'>              Optional: false
      MYSQL_PASSWORD:              <set to the key 'mysql-password' in secret 'mysql-secret'>                     Optional: false
      MYSQL_ALLOW_EMPTY_PASSWORD:  <set to the key 'mysql-allow-empty-password' of config map 'mysql-configmap'>  Optional: false
      MYSQL_ROOT_HOST:             <set to the key 'mysql-root-host' of config map 'mysql-configmap'>             Optional: false
    Mounts:
      /etc/mysql/conf.d from mysql-conf (rw)
      /var/lib/mysql from mysql-data (rw,path="mysql")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m45lh (ro)
  xtrabackup:
    Container ID:  
    Image:         gcr.io/google-samples/xtrabackup:1.0
    Image ID:      
    Port:          3307/TCP
    Host Port:     0/TCP
    Command:
      bash
      -c
      set -ex
      cd /var/lib/mysql
      
      if [[ -f xtrabackup_slave_info && "x$(<xtrabackup_slave_info)" != "x" ]]; then
        cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in
        rm -f xtrabackup_slave_info xtrabackup_binlog_info
      elif [[ -f xtrabackup_binlog_info ]]; then
        [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
        rm -f xtrabackup_binlog_info xtrabackup_slave_info
        echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
              MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
      fi
      
      if [[ -f change_master_to.sql.in ]]; then
        echo "Waiting for mysqld to be ready (accepting connections)"
        until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
      
        echo "Initializing replication from clone position"
        mysql -h 127.0.0.1 \
              -e "$(<change_master_to.sql.in), \
                      MASTER_HOST='mysql-0.mysql', \
                      MASTER_USER='root', \
                      MASTER_PASSWORD='', \
                      MASTER_CONNECT_RETRY=10; \
                    START SLAVE;" || exit 1
        mv change_master_to.sql.in change_master_to.sql.orig
      fi
      
      exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
        "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/mysql/conf.d from mysql-conf (rw)
      /var/lib/mysql from mysql-data (rw,path="mysql")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m45lh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 False 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  mysql-conf:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  mysql-configmap:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      mysql-configmap
    Optional:  false
  mysql-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mysql-pvc
    ReadOnly:   false
  kube-api-access-m45lh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  17m                  default-scheduler  0/3 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
  Warning  FailedScheduling  14m                  default-scheduler  0/3 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
  Normal   Scheduled         14m                  default-scheduler  Successfully assigned default/mysql-statefulset-0 to worker1
  Warning  FailedMount       2m1s (x14 over 14m)  kubelet            MountVolume.SetUp failed for volume "pvc-e82567f0-8a61-4837-a721-8065d902c885" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs 192.168.56.21:/srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 /var/lib/kubelet/pods/4599f8fa-54c7-48d4-b246-413f9fb7edc7/volumes/kubernetes.io~nfs/pvc-e82567f0-8a61-4837-a721-8065d902c885
Output: mount.nfs: mounting 192.168.56.21:/srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 failed, reason given by server: No such file or directory



agrant@control-plane1:~$ kubectl get pv pvc-e82567f0-8a61-4837-a721-8065d902c885 -o yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: nfs-provisioner
  creationTimestamp: "2025-04-05T08:59:49Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-e82567f0-8a61-4837-a721-8065d902c885
  resourceVersion: "6970"
  uid: ad5812bc-5f8a-4092-a8c5-0af81454dcb9
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 5Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: mysql-pvc
    namespace: default
    resourceVersion: "6533"
    uid: e82567f0-8a61-4837-a721-8065d902c885
  nfs:
    path: /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885
    server: 192.168.56.21
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs
  volumeMode: Filesystem
status:
  lastPhaseTransitionTime: "2025-04-05T08:59:49Z"
  phase: Bound


명해 드리겠습니다.

분석 결과:

파드 상태: 파드 mysql-statefulset-0는 현재 Pending 상태입니다. 이는 파드가 아직 노드에서 완전히 시작되지 못했음을 의미합니다. 컨테이너들이 Waiting 상태이고 이유는 PodInitializing으로 표시되는데, 이는 파드 실행에 필요한 사전 준비(특히 볼륨 마운트) 단계에서 문제가 발생했음을 시사합니다.
스케줄링: 이벤트 로그를 보면, 처음에는 FailedScheduling 경고가 발생했습니다. 원인은 pod has unbound immediate PersistentVolumeClaims로, mysql-pvc라는 PVC가 PV(PersistentVolume)에 바인딩되지 않아 파드를 스케줄링할 수 없었기 때문입니다.
스케줄링 성공: 약 14분 전에 default-scheduler가 파드를 worker1 노드에 성공적으로 할당(Successfully assigned)했습니다. 이는 그 시점에 mysql-pvc가 PV에 성공적으로 바인딩되었음을 의미합니다.
마운트 실패 (현재 문제): 파드가 worker1 노드에 할당된 직후부터 FailedMount 경고가 반복적으로 (x14 over 14m) 발생하고 있습니다. 이것이 파드가 Pending 상태에 머물러 있는 주요 원인입니다.
오류 내용: MountVolume.SetUp failed for volume "pvc-e82567f0-8a61-4837-a721-8065d902c885" : mount failed: exit status 32
세부 정보: Kubelet (워커 노드의 에이전트)이 mysql-data 볼륨 (PVC mysql-pvc에 해당)을 마운트하려고 시도 중입니다. 이 볼륨은 NFS 유형으로 보입니다.
시도된 명령어: mount -t nfs 192.168.56.21:/srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 /var/lib/kubelet/pods/4599f8fa...
서버 응답: mount.nfs: mounting ... failed, reason given by server: No such file or directory
결론:

파드 mysql-statefulset-0가 시작되지 못하는 이유는 worker1 노드가 mysql-data 퍼시스턴트 볼륨을 마운트하는 데 실패했기 때문입니다. 구체적으로, worker1 노드가 NFS 서버 192.168.56.21에 있는 /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 경로를 마운트하려고 했지만, NFS 서버 측에서 해당 경로(디렉토리)가 존재하지 않는다고 응답했습니다.

문제 해결을 위한 다음 단계:

NFS 서버(192.168.56.21) 설정을 확인해야 합니다.

NFS 서버 접속: 192.168.56.21 IP 주소를 가진 NFS 서버에 직접 접속합니다.
경로 확인: 서버에서 /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 디렉토리가 실제로 존재하는지 확인합니다. 존재하지 않을 가능성이 매우 높습니다.
PV 정의 확인: mysql-pvc에 바인딩된 PV(PersistentVolume)의 설정을 확인하여 NFS 경로가 올바르게 지정되었는지 확인합니다. PV 이름은 이벤트 로그의 볼륨 이름(pvc-e82567f0-8a61-4837-a721-8065d902c885)일 가능성이 높습니다.
Bash

kubectl get pv pvc-e82567f0-8a61-4837-a721-8065d902c885 -o yaml
위 명령어로 PV의 spec.nfs.path가 NFS 서버에 실제로 존재하는 경로와 일치하는지 확인하십시오.
NFS Export 설정 확인: NFS 서버의 export 설정 파일 (보통 /etc/exports)을 확인하여 해당 디렉토리 또는 상위 디렉토리가 worker1 노드 (192.168.56.22)에서 접근 가능하도록 올바르게 export 되었는지 확인합니다.
디렉토리 생성 또는 경로 수정:
만약 PV가 동적으로 생성되었고 해당 디렉토리가 자동으로 생성되지 않았다면, NFS 서버에 해당 경로를 수동으로 생성해야 할 수 있습니다. (예: mkdir -p /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885)
만약 PV의 경로 설정이 잘못되었다면, PV 설정을 수정하거나 (어렵다면 PV/PVC를 재생성하고) NFS 서버에 존재하는 올바른 경로를 사용하도록 변경해야 합니다.
NFS 서버 서비스 확인: NFS 서버 데몬(서비스)이 정상적으로 실행 중인지 확인합니다.
요약하자면, NFS 서버(192.168.56.21)에 워커 노드가 마운트하려는 디렉토리가 없거나 접근 권한 문제가 있는 것이 원인입니다. NFS 서버 설정을 확인하고 수정해야 합니다.



알겠습니다. 제공해주신 NFS 서버의 /etc/exports 파일 내용을 확인했습니다.

/srv/nfs *(rw,sync,no_subtree_check)
분석:

이 설정은 NFS 서버의 /srv/nfs 디렉토리를 모든 클라이언트 IP(*)에 대해 읽기/쓰기(rw) 권한으로 공유(export)한다는 의미입니다. sync, no_subtree_check는 일반적인 옵션입니다.
설정 자체는 worker1 노드(192.168.56.22)가 /srv/nfs 디렉토리에 접근하는 것을 허용합니다. 따라서 /etc/exports 설정 자체는 문제가 아닌 것으로 보입니다.
문제의 핵심:

이전 kubectl describe pod 결과에서 확인된 오류 메시지를 다시 보면, 문제는 worker1 노드가 마운트하려는 특정 하위 디렉토리가 NFS 서버에 없다는 것입니다.

마운트 시도 경로: 192.168.56.21:/srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885
서버 응답: No such file or directory
즉, NFS 서버(192.168.56.21)의 /srv/nfs 디렉토리는 공유되었지만, 그 안에 default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885 라는 이름의 디렉토리가 존재하지 않기 때문에 마운트에 실패하는 것입니다.

가장 가능성 높은 원인:

이러한 상황은 보통 NFS 동적 프로비저너(Dynamic Provisioner) (예: nfs-subdir-external-provisioner)를 사용할 때 발생합니다. 프로비저너는 PVC(PersistentVolumeClaim) 요청이 들어오면 NFS 서버의 공유 디렉토리 아래에 자동으로 고유한 하위 디렉토리를 생성하고, 이 경로를 사용하는 PV(PersistentVolume)를 만들어야 합니다.

현재 오류는 이 자동 디렉토리 생성이 실패했거나 제대로 이루어지지 않았음을 시사합니다. PV 객체는 생성되었지만(kubectl describe pod의 Scheduled 이벤트 참고), 해당 PV가 가리키는 실제 디렉토리가 NFS 서버에 없는 것입니다.



vagrant@control-plane1:~$ kubectl logs -n default nfs-provisioner-deployment-6f5cc98c6f-fm7zk

I0405 08:59:48.994955       1 leaderelection.go:242] attempting to acquire leader lease  default/nfs-provisioner...
I0405 08:59:49.008406       1 leaderelection.go:252] successfully acquired lease default/nfs-provisioner
I0405 08:59:49.008861       1 controller.go:820] Starting provisioner controller nfs-provisioner_nfs-provisioner-deployment-6f5cc98c6f-fm7zk_628385c1-ca76-4dbd-8eda-d996a86787ee!
I0405 08:59:49.009355       1 event.go:278] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"default", Name:"nfs-provisioner", UID:"381d9dc9-1246-4dcc-93a2-e0346f49e96c", APIVersion:"v1", ResourceVersion:"6955", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' nfs-provisioner-deployment-6f5cc98c6f-fm7zk_628385c1-ca76-4dbd-8eda-d996a86787ee became leader
I0405 08:59:49.109184       1 controller.go:869] Started provisioner controller nfs-provisioner_nfs-provisioner-deployment-6f5cc98c6f-fm7zk_628385c1-ca76-4dbd-8eda-d996a86787ee!
I0405 08:59:49.109634       1 controller.go:1317] provision "default/mysql-pvc" class "nfs": started
I0405 08:59:49.109779       1 controller.go:1317] provision "default/jenkins-pvc" class "nfs": started
I0405 08:59:49.114409       1 controller.go:1420] provision "default/jenkins-pvc" class "nfs": volume "pvc-30cf7541-bd8a-4ca7-9b05-474e76849697" provisioned
I0405 08:59:49.114442       1 controller.go:1437] provision "default/jenkins-pvc" class "nfs": succeeded
I0405 08:59:49.114451       1 volume_store.go:212] Trying to save persistentvolume "pvc-30cf7541-bd8a-4ca7-9b05-474e76849697"
I0405 08:59:49.115334       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"jenkins-pvc", UID:"30cf7541-bd8a-4ca7-9b05-474e76849697", APIVersion:"v1", ResourceVersion:"6531", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/jenkins-pvc"
I0405 08:59:49.199322       1 controller.go:1420] provision "default/mysql-pvc" class "nfs": volume "pvc-e82567f0-8a61-4837-a721-8065d902c885" provisioned
I0405 08:59:49.199377       1 controller.go:1437] provision "default/mysql-pvc" class "nfs": succeeded
I0405 08:59:49.199387       1 volume_store.go:212] Trying to save persistentvolume "pvc-e82567f0-8a61-4837-a721-8065d902c885"
I0405 08:59:49.199898       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"e82567f0-8a61-4837-a721-8065d902c885", APIVersion:"v1", ResourceVersion:"6533", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mysql-pvc"
I0405 08:59:49.428372       1 volume_store.go:219] persistentvolume "pvc-30cf7541-bd8a-4ca7-9b05-474e76849697" saved
I0405 08:59:49.437045       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"jenkins-pvc", UID:"30cf7541-bd8a-4ca7-9b05-474e76849697", APIVersion:"v1", ResourceVersion:"6531", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-30cf7541-bd8a-4ca7-9b05-474e76849697
I0405 08:59:49.806780       1 volume_store.go:219] persistentvolume "pvc-e82567f0-8a61-4837-a721-8065d902c885" saved
I0405 08:59:49.807801       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mysql-pvc", UID:"e82567f0-8a61-4837-a721-8065d902c885", APIVersion:"v1", ResourceVersion:"6533", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-e82567f0-8a61-4837-a721-8065d902c885




Mounting command: mount
Mounting arguments: -t nfs 192.168.56.21:/srv/nfs/default-jenkins-pvc-pvc-30cf7541-bd8a-4ca7-9b05-474e76849697 /var/lib/kubelet/pods/93a131d7-2d31-4f67-8a91-642cafaa579d/volumes/kubernetes.io~nfs/pvc-30cf7541-bd8a-4ca7-9b05-474e76849697
Output: mount.nfs: mounting 192.168.56.21:/srv/nfs/default-jenkins-pvc-pvc-30cf7541-bd8a-4ca7-9b05-474e76849697 failed, reason given by server: No such file or directory

# 1. 디렉토리 생성 (-p 옵션은 상위 디렉토리도 필요시 생성)
kubectl describe pod mysql-statefulset-0

sudo mkdir -p /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885

# 2. 소유권 설정 (이전 권한 확인 결과 nobody:nogroup 이 적절해 보임)
sudo chown nobody:nogroup /srv/nfs/default-mysql-pvc-pvc-e82567f0-8a61-4837-a721-8065d902c885

kubectl get pod mysql-statefulset-0 


kubectl describe pod jenkins-statefulset-0

# 1. 디렉토리 생성 (-p 옵션은 상위 디렉토리도 필요시 생성)
sudo mkdir -p /srv/nfs/default-jenkins-pvc-pvc-30cf7541-bd8a-4ca7-9b05-474e76849697

# 2. 소유권 설정 (이전 권한 확인 결과 nobody:nogroup 이 적절해 보임)
sudo chown nobody:nogroup /srv/nfs/default-jenkins-pvc-pvc-30cf7541-bd8a-4ca7-9b05-474e76849697

kubectl get pod jenkins-statefulset-0